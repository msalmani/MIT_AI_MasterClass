---
title: "Data Preparation and Visualization"
author: "Mehdi Salmani Jelodar - BCG"
output: html_document
---

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(ggplot2) #graphics library
library(corrplot) #correlation charts
library(devtools) #required by bcgstyle
library(rpart) # CART model
library(C50) # C50 and Rule-Based classification models
library(randomForest) #Random forest model
library(neuralnet) #Neural network model
library(pROC) # ROC curve and AUC metric
data(churn)
```


## Data preparation and cleaning

Data preparation and cleaning include shaping the format of the data, removing highly correlated columns, taking care of extreme values/outliers, fixing the range of the data for some special use cases, de-biasing the data, imputing missing data.

This dataset has been already processed and prepared for the analysis. We will focus our analyses to understand the distribution of the data, and to identify additional manipulations required to use the data in specific methods, e.g. neural networks.

The correlation plot shows that "charge" and "minute" variables follow exactly the same pattern (most likely "charge" is a multiple of "minute"). One of these groups can be removed from the analysis without loss of information.

```{r}
nums <- sapply(churnTrain[,1:19], is.numeric) # Select numeric variables for correlation plot mapping
corrplot(cor(churnTrain[,nums]), tl.cex = 0.5, cl.cex = 0.5, type="lower")
```


## Exploratory analysis

The analysis of the distribution for some of the variables in the dataset offers some preliminary insights on what variables, and especially specific ranges, could drive higher attrition. The higher concentration of "churn" clients is evident when analyzing the stacked bar charts. 

Higher values of "Customer Service Calls" and "Total Day Minutes" are characterized by a higher concentration of clients that churned.

On the other hand, the top accounts in term of "length" of the relationship are unlikely to churn.

Despite some evidence of higher churn concentration in some states, the "state" variable should be dropped because the number of observations per each state is not enough to support meaningful analyses and conclusions.

```{r}
# Histogram for account length, customer service call and total day minutes
# Use geom_histogram, for continuous variables
ggplot(data = churnTrain, aes(x=account_length)) + geom_histogram(position = "fill", binwidth = 10, aes(fill=churn)) + labs(title = "Account Length Distribution Frequency", x = "Account Length", y = "Frequency")
ggplot(data = churnTrain, aes(x=total_day_minutes)) + geom_histogram(position = "fill", binwidth = 10, aes(fill=churn)) + labs(title = "Total Day Minutes Distribution Frequency", x = "Total Day Minutes", y = "Frequency")
ggplot(data = churnTrain, aes(x=number_customer_service_calls)) + geom_histogram(position = "fill", binwidth = 1, aes(fill=churn)) + labs(title = "Customer Service Calls Distribution Frequency", x = "Customer Service Calls", y = "Frequency")

# By plotting the variables as stacked bars normalized at 100% for each bin, we can see interesting patterns emerging
ggplot(data = churnTrain, aes(x=account_length)) + geom_histogram(position = "fill", binwidth = 10, aes(fill=churn)) + labs(title = "Account Length Distribution Frequency", x = "Account Length", y = "Frequency")
ggplot(data = churnTrain, aes(x=total_day_minutes)) + geom_histogram(position = "fill",binwidth = 10, aes(fill=churn)) + labs(title = "Total Day Minutes Distribution Frequency", x = "Total Day Minutes", y = "Frequency")
ggplot(data = churnTrain, aes(x=number_customer_service_calls)) + geom_histogram(position = "fill",binwidth = 1, aes(fill=churn)) + labs(title = "Customer Service Calls Distribution Frequency", x = "Customer Service Calls", y = "Frequency")

# Histogram for area code, international plan and voice plan
# Use geom_bar, for discrete variables
ggplot(data = churnTrain, aes(x=area_code)) + geom_bar(position = "fill", aes(fill=churn)) + labs(title = "Area Code Distribution Frequency", x = "Area Code", y = "Frequency")

# There are certain states with higher churn, however the low number of observations per state suggests that this variable should be removed
ggplot(data = churnTrain, aes(x=state)) + geom_bar(position = "fill", aes(fill=churn)) + labs(title = "State Distribution Frequency", x = "State", y = "Frequency")+ theme(text = element_text(size=8),axis.text.x = element_text(angle=90, vjust=1))

```
```{r echo=FALSE,eval=FALSE}
#ggplot(data = churnTrain, aes(x=state)) + geom_bar(position="fill",aes(fill=churn)) + labs(title = "State Distribution Frequency", x = #"State", y = "Frequency")+ theme(text = element_text(size=8),axis.text.x = element_text(angle=90, vjust=1))
```


## Preparing test and train dataset for modeling 
Based on these preliminary analyses, it is now possible to finalize the preparation of the data set. We focus on 
* removing redundant and unusable variables identified earlier
* creating data sets with independent and dependent variables for both training and test
* initializing data structures used to keep track of the different models built and their performance

```{r}
# Create train and test data sets for independent variables
# Drop "state" and "churn" variables, as well as "charge" variables with perfect positive correlation
var.out <- c("state", "churn", "total_day_charge", "total_eve_charge",  "total_night_charge",  "total_intl_charge")
var.out.index <- which(names(churnTest) %in% var.out)
churnTest_x <- churnTest[,-var.out.index]
churnTrain_x <- churnTrain[,-var.out.index]

# Create train and test data sets for target variable "churn"
churnTest_y <- churnTest[,c("churn")]
churnTrain_y <- churnTrain[,c("churn")]
y_Test_Table <- table(churnTest_y)
y_Train_Table <- table(churnTrain_y)

# Initialize data structures for storing models information and performance
Confusion_Matrix <- list()
Models_specs <- vector()
Num_models <- 0
```

